# Interview Five：集合框架

- Map
    - HashMap
        - 哈希函数
        - 解决哈希冲突（底层结构）：数组、链表、红黑树
        - put/resize()，尾插法和头插法
        - 特性：无序，空间复杂度换时间复杂度
    - LinkedHashMap：LRU
    - ConcurrentHashMap

- ArrayList和LinkedList

    - 数据结构：Object[]数组连续空间，双向链表不连续空间

    - 随机访问性：ArrayList具备，LinkedList不具备

    - 插入/删除开销：ArrayList的add方法默认往数组尾部添加，这是因为在数组中间位置添加需要挪动后续元素的位置；链表直接修改引用就行

    - 内存占用情况：链表需要保存前/后指针

    - 都是线程不安全的

# **1. HashMap**

**底层结构**：`数组 + 链表/红黑树`存储键值对，通过映射关系链接起**键和数组索引的关系**，使用拉链法解决哈希冲突问题

**哈希函数**：计算出键值在数组的对应位置

- 计算公式：`hash & (size - 1)`

    - hash：key的hashcode，调用key.hashCode()方法获得
    - size：数组的长度
    - size - 1：比特位全为1的掩码
    - 结果：size - 1作为掩码，与元素hash值做与运算，得到值一定在size的范围之内，也就是最终下标

- 公式思路：取key的hash值特征来得出**下标索引**，& 运算比 % 运算有更低的计算开销，但是要注意

    - 数组长度应为**2的幂次方**

    - 对于低位特征不明显（基本全是1或者0）的key，在基本数组长度都较小的情况下，分布将不够均匀，所以HashMap还会通过将hash值的**高16位和低16位**进行”异或“运算

- `哈希冲突`：指数组长度有限，即使key的hash不同，通过公式计算仍可能处于同一个数组索引位导致冲突

    - 解决思路：采用拉链法解决该问题，具体采用长度小于8使用链表，**长度大于等于8且桶数组长度达到64时**转换链表为红黑树

- 扩容时，同一个桶内的元素会迁移至新链表的老索引位置，或新链表的{老索引+oldCap}位置，具体由以下公式计算而成

    - oldCap & e.hash == 0：保留在老位置
    - oldCap & e.hash != 0: 迁移至新位置

### 1. 为什么用红黑树作为优化点？

加快检索速度，桶长度过长时，变成了线性查找，hashmap的性能下降严重

### 2. 为什么要有阈值，且阈值为8（为什么不直接使用红黑树）？

- 为什么不直接使用红黑树

    1. **链表转换为红黑树的过程需要耗时**

    2. 红黑树保证树的平衡度，但增删操作涉及到左、右旋操作，导致增删桶元素的成本变高

- 为什么要有阈值

    通过阈值控制使用链表和红黑树，可以结合链表和红黑树的特性，对**查找成本和新增元素成本进行权衡**：

    当元素较少时使用链表，查询成本不至于太高，且其新增元素成本低；当元素超过阈值时使用红黑树，降低链表的查询的成本

- 为什么阈值是8？

    是对性能的权衡，对查询、转化、新增成本的权衡。

    首先，hashmap中链表才是常态，**链表长度小于8时，即使全部遍历，时间复杂度也不会很高**。

    其次，阈值的设置也有学问。
    - 阈值设置过小，可能导致链表和红黑树频繁转化，树化和退化之间转化的开销不容小觑。
    - 阈值设置过大，又导致没有及时树化，链表过长降低了查询性能。
    
    hash冲突的次数与泊松分布相关，桶内结点数量为8的概率小于百万分之一，桶内元素的大于等于8的概率极低。所以阈值设置为8，可以**尽量避免红黑树的出现**。

### **3. 为什么转换为红黑树的条件，需要桶节点数量为8，还要保证数组长度到达64？**

在没到达64长度前，扩容比转化红黑树对解决hash冲突的效率更高；相反的，**`如果数组过长，进行一次扩容的数据更多`**，比转化为红黑树的效率低

### **4. 为什么使用红黑树，而不用AVL树？**

插入Node引起树的不平衡：AVL和RB-Tree都是最多只需要2次旋转操作，两者都是O(1)

删除Node引起树的不平衡：

- AVL需要维护被删node到root这条路径上所有node平衡性，量级为O(logN)

- RB-Tree最多只需要3次旋转，量级为O(1)

查找效率：

- AVL结构更为平衡，适合频繁查找的场景，但在频繁增删的场景下AVL需要的rebalance量级更高

- RB-Tree结构是非严格平衡的二叉查找树（**保证最长路径不超过最短路径的两倍**），在频繁增删的场景下效率更高，查找

    > [但是在与AVL树做比较的时候，都说红黑树的统计性能更好](https://www.zhihu.com/question/43744788/answer/98258881)：rb-tree比avl树会稍微不平衡最多一层是不对的，应为：红黑树的查询性能略微逊色于AVL树，因为他比avl树不平衡约n层。n为黑色node个数，但因为O(2logn)复杂度仍为O(logn) 所以仍然算作平衡树的一种

总结：HashMap的实现折衷**search、insert、delete三种操作的效率**，RB-Tree在频繁增删的场景下效率更高，而查找效率最差情况为最长路径接近最短路径的2倍，整体来说RB-Tree的统计性能高于AVL

### **5. 为什么有退化？退化的阈值为什么是6？**

为什么要转换为红黑树？又为什么要退化成链表？
- 为什么转换为红黑树：为了解决链表过长导致的查找效率低下问题，将时间复杂度从O(N)降为O(logN)，但是随之引入了插入、删除时维护红黑树平衡的成本。
- 为什么退化：在链表长度为6的时，链表和红黑树的查找效率差距并没有很大，退化反而可以省去红黑树持续维护的成本。红黑树节点较大，空间复杂度较高。

为什么转换的阈值是8？
- 因为在默认的负载因子，遵循泊松分布，链表长度达到8的概率接近百万分之一，当真正出现时说明哈希分布不均，此时引入红黑树提升查询效率的收益大于引入红黑树的维护成本。

为什么退化链表的阈值为6？
- 链表长度为6的时，链表和红黑树的查找效率差距并没有很大。其次，8和6之间有**缓冲区**，防止极端增删情况下导致的频繁树化和退化。

### **6. resize()为什么使用(e.hash & oldCap == 0)来分配位置？**

为什么通过(e.hash&oldCap)==0来判断新的索引位置
这种方式更巧妙一点，我个人理解用原始计算索引公式也是一样的，以下为推导过程

首先索引计算公式：e.hash&(n-1)

2倍扩容，二进制相较于原始容量二进制高位多个1；我们知道&运算结果只有两个：0/1 ；只有都为1时才为1，其余都为0；原始容量高位为0，那么&结果一定是0；扩容后该高位为1那么计算新索引位置就有两种可能：1、运算结果仍然为0，那么就新的索引位置仍然为原始位置 2、结果为1，那么新位置=原始位置+oldCap

举例：原始容量32；a = 5,b = 37 两节点处于同一索引位置；扩容后我们看到a索引位置没变 b索引位置 = 原索引位置+oldCap

- [为什么通过(e.hash&oldCap)==0来判断新的索引位置](https://zhuanlan.zhihu.com/p/655228233)

## **API**

**put**：

1. 调用HashMap的hash方法

    - 调用key的hashCode()方法获得哈希值
    - 将其高16位和低16位异或操作得到更具特征的哈希值

2. 通过哈希函数key & (size - 1)得出哈希值对应数组的下标值

3. 判断当前数组位置是否有元素，无则直接存入，有则遍历该桶，有相等key则替换，否则将新元素插入到链表的尾部（尾插法）

4. 最后判断**键值对个数**是否超过阈值，阈值通过`size数组长度 * factor负载因子`计算得出，若超过则执行resize()扩容

**get**：

1. 同样根据put过程得到最终数组下标索引

2. 根据桶的数据结构进行查询

**resize**：

1. 创建一个当前数组**两倍长度**的新数组

2. key重新计算hash，采用头插法的方式将数据拷贝到新数组上

    元素rehash后的结构，只会在当前位置，或当前位置+旧数组长度，这是由于哈希函数使用的是“&”运算

    - 当前位置公式：oldCap & hash == 0
    
        (2oldCap - 1) & hash = (oldCap - 1) & hash

        => 2oldCap & hash - 1 & hash = oldCap & hash - 1 & hash

        => 2(oldCap & hash) = oldCap & hash

        => oldCap & hash = 0

**扩容先后问题**：
- HashMap是集中式扩容，实现上采用**先插入后扩容**（但本质上扩容和插入的先后没有很大关系）
    - 实现思路：一次性迁移
    - 优点：追求迁移效率
    - 缺点：牺牲可用性，迁移期间服务不可用
    
- Redis是渐进式扩容，实现上采用**先扩容再插入**
    - 实现思路：分批次迁移，迁移期间创建双数组。当查询遇到对应的元素时，则将元素对应的桶，从旧数组搬到新数组中，再最终返回结果
    - 优点：将迁移开销分摊到每次操作中，适用于高并发系统，避免服务长时间阻塞
    - 缺点：迁移效率过慢，如果频繁触发扩容将
    - 优化措施：
        - 1. 后台定时任务加速迁移：默认每100ms运行后台任务，主动迁移一批数据（默认10个桶）
        - 2. 写操作定向至新数组
        - 3. 负载因子阈值优化，根据数据波动情况进行动态调整，防止极端情况下波动过大导致频繁落日
        - 4. 引导客户端重试操作，限流
        - 5. 动态调整迁移粒度：根据负载自动控制单次迁移的桶数量，在高频扩容时优先迁移热点数据桶

**头插法死循环**：有两个因素，一个是迭代旧桶和插入新桶的顺序不一致，另外一个是并发环境下触发了两次resize()

    两个线程各自resize()的过程，对某个桶的链表的迁移过程出现了闭环
    
    顺序不一致：线程获取的结点顺序是正序，而头插法插入到新桶的顺序是倒序的

    并发触发两次resize：A线程可能获取到旧桶的顺序后还未开始迁移到新桶时被暂停，而B线程完成了迁移过程，当A线程重新获得cpu时间片完成后续resize的迁移过程后产生闭环

- 正序遍历：从桶头到桶尾遍历

- 正序插入（头插法）：将新结点直接插入到桶头位置，旧桶头作为其下一个节点

- 倒序插入（尾插法）：新结点直接插入到桶尾

- [1.7HashMap死循环](https://cloud.tencent.com/developer/article/2111153)
- [1.7HashMap死循环问题动态图](https://ask.qcloudimg.com/http-save/yehe-8613088/76502bc0fd9aece30f8a5bd6204467b4.gif)

# **2. LinkedHashMap**

底层结构：双向链表 + 哈希表

用途：在固定大小下，可作为LRU缓存。

流程：每次查询、新增某个元素时，都将该元素移动至双向链表的头部，且当新增的数量超过固定大小时，淘汰掉链表尾部的元素

```java
class LRUCache extends LinkedHashMap<Integer, Integer> {
    private int capacity;

    public LRUCache(int capacity) {
        super(capacity, 0.75f, true)
        this.capacity = capacity;
    }

    public int get(int key) {
        return super.getOrDefault(keu. -1);
    }

    public void put(int key, int value) {
        super.put(key, value);
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry<Integer, Integer> eldest) {
        //  大于固定大小时，淘汰链表尾部的元素
        return size() > capacity;
    }
}
```

LeetCode 146.LRU缓存实现思路：

- 公有方法：
    - get：根据哈希表查询到元素，将其移动至链表头部
    - put：根据哈希表查询到元素
        - 若有元素，将元素移动至链表头部，覆盖其值，若超过容量，需要删除链表尾部
        - 若无元素，则添加元素到头部，若超过容量，需要删除链表尾部元素

- 私有方法：
    - removeTail
    - addToHead
    - removeNode
    - moveToHead

# **3. ConcurrentHashMap**

HashTable、Collections.synchronizedMap同步加锁可以解决线程安全问题，但是锁粒度过大性能过差，ConcurrentHashMap更适合高并发场景使用

- JDK7：Segment + HashEntry的分段锁实现，底层结构的最外层是一个Segment数组，每个Segment元素中包含一个HashEntry数组

    - 结构：锁粒度是Segment

        - Segment：Segment继承自ReentrantLock

        - HashEntry：是一个链表结构，具有保存key、value的能力和指向桶下一个结点的指针（final指针）

        相当于每个Segment都是一个HashTable，默认的Segment长度是16，**并发写的程度是16个线程**

        > 所以每个键值对都需要经过**两次hash**来找到最终的数组位置，一次是定位Segment，一次是定位Segment对应位置

    - 操作

        - put：两次hash计算、自旋尝试（默认2次）、操作时加锁

            1. 第一次hash函数计算，定位到hash对应具体的Segment，如果Segment为空则初始化

            2. 使用ReentrantLock尝试加锁，失败则**尝试自旋**（默认2次），自旋超过一定次数则阻塞挂起

            3. 获取到锁后，第二次hash函数计算，计算HashEntry数组对应的HashEntry，再遍历HashEntry链表

        - remove：操作时加锁、删除结点前可能会有复制开销
        
            复制删除结点之前的结点（next指针final），final的next指针意味着无需加锁就可以遍历，这种做法默认加锁成本大于每次删除的复制开销，不适用于频繁删除场景

        - get：最终一致性，类似读快照的方式
        
            整个结构的域基本都是volatile，通过类似count的结构变量来感知集合的变化，以尽量做到无锁化

        - resize：在put过程中扩容（先插入再扩容），跟put操作的锁是同一个

- JDK8：改为使用CAS + synchronized实现，包括了一个Node数组，synchronized的**监视器为桶头结点**

    - 结构：并发粒度为Node

        - Node：包含key、value、next属性，都为**volatile类型**的，以保证在线程之间的可见性

        - sizeCtl：用于协调容器的某些过程（初始化、扩容）和记录当前键值对元素的数量

            - -1：表示当前的Node桶正在初始化

            - \>0：表示当前容器的扩容阈值

            - < -1：表示当前容器正在扩容

    - 操作：

        - put：CAS+自旋初始化桶，桶头采用CAS写入、扩容辅助、桶其他结点使用synchronized写入

            1. 计算hash，方式与HashMap类似，不过会将符号位采用HASH_BITS去除掉最高位，因为桶头元素的负hash值有其他含义（-1/-2/-3）

            2. 如果表没有初始化，则先通过对sizeCtl进行CAS设置为-1（自旋），保证同一时间只有一个线程执行初始化逻辑。

            3. 如果表已初始化，则计算出元素对应的数组下标位置，将元素添加到桶中。
            
            - 如果桶为空，通过CAS设置，成功则后续**进入addCount扩容逻辑**；失败则重新进入自旋逻辑。

            - 如果桶不为空，桶头hash值不为-1，则直接使用监视器锁锁定当前桶，将元素加入到桶中，并**进入判断addCount扩容判断**；桶头hash值为-1，表示当前容器正在执行扩容逻辑，调用`helpTransfer()`方法辅助扩容

        - remove：
            - 扩容辅助
            - 通过synchronized监视器桶，遍历桶中要删除的元素，并通过CAS的方式覆盖next指针，将链表元素删除。

        - transfer：当前数量超过sizeCtl触发；或者有链表的长度到达8，且当前数组长度还未超过64时触发。`ForwardingNode标识当前桶已处于迁移状态`
        
        1. 扩容会将数组分成多个分段，每个分段大小通过当前进程的cpu个数进行计算，最小为16
        
        2. 每个线程通过transferIndex和分段大小来对自己的区域进行迁移，迁移线程之间互不影响通过**sizeCtl**来进行协调
        
        3. 最后一个完成的线程会做容器的扩容收尾工作，如重新设置sizeCtl为扩容后的新阈值

        4. 扩容的算法：计算lastRun，将lastRun作为新桶（新旧位置不确定）的头结点，并重新遍历当前旧桶的数据，与lastRun高位不同的依次放入另一个链表中

        ![ConcurrentHashMap扩容算法](https://asea-cch.life/upload/2022/01/ConcurrentHashMap%E6%89%A9%E5%AE%B9%E7%AE%97%E6%B3%95-e664284acb5b47689bc06f79849d5aae.png)

        > get操作不会触发扩容

> JDK8的concurrentHashMap特征：`粒度缩小`（桶头的状态），`扩容新增辅助`的概念（需要考虑扩容过程中出现其他操作的情况），`扩容粒度缩小`（可以在某种程度上边操作容器边扩容），`get()新增转发结点概念`

问题1：扩容期间可以插入数据吗？

可以，只要插入的位置还没有被扩容线程迁移到，就可以插入，与此同时同时迁移线程到达插入位置准备迁移时，会阻塞等待插入数据完成后再进行迁移

以上两个过程都通过CAS / synchronized进行互斥，保证线程安全

问题2：扩容期间可以查询数据吗？

JDK7的方式是直接查询旧数组，**时效性较差**

通过final对next指针进行限定，最后再进行全桶替换，保证不受到并发扩容导致的影响

JDK8更为实时，分为两个情况：

- 当前桶结点处于正在迁移的过程中，桶头还未设置为ForwardingNode

    新形成的hn和ln链最后通过CAS设置到**新数组**上，即没有修改原本的数组，所以可以读到之前的hash桶上的链表

- 当前桶结点已经迁移完毕，桶头为ForwardingNode

    get方法会调用`结点的find()`，而ForwardingNode的find方法则是**转发到新的数组上**进行搜索

问题3：为什么桶为null用CAS，桶不为空是synchronized

桶为null意味着竞争不激烈

问题4: ConcurrentHashMap的size()方法为何不精确？有没有替代的方案？

为了保证并发环境下的性能，size()方法并没有全局锁定来保证size的绝对精准，所以存在多个线程在统计计数器（CounterCell）的同时插入删除元素，导致结果只能是一个趋向精确的近似值。

首先一定要在业务上确认是否必须需要如此精确的值，其次有如下解决方案：
1. 全局锁定（不推荐）
2. 维护一个原子计数器，在ConcurrentHashMap对象上的元素增删入口进行全方位的统计（确保元素添加、删除成功再统计）

# **4. ArrayList和LinkedList的区别**

- 底层数据结构

    ArrayList底层是一个`Object数组`，是连续的内存空间
    
    LinkedList底层是一个`双向链表`，不具备连续内存空间

- 随机访问性（内存分布情况）

    ArrayList由于是`连续的内存空间`，具备随机访问性，可以直接通过`索引下标`计算出Base + Offset的位置访问内存

    LinkedList的底层结构是`不连续的`，无法提供随机访问性

    > 遍历LinkedList要用迭代器而不是for循环指定i，后者会产生n * (n - 1) * ... * 1的性能开销

- 插入和删除开销（对于已经找定某个位置下元素）：

    ArrayList：采用数组存储，所以插入和删除元素的时间复杂度受`元素位置`的影响：
    
    - 如果在指定位置添加和删除元素的话，其之后的元素都需要往前/后挪动，所以`add()操作是直接往数组末尾添加`
    
    - 当底层数组不够存储时扩容，ArrayList会计算添加个数与当前元素个数之和，并与当前底层数组大小的进行对比，不够时则扩张1.5倍

    LinkedList：采用链表，插入，删除元素时间复杂度不受元素位置影响，直接将指针改一下就好了，时间复杂度为O(1)

- `内存占用空间`：链表的元素要存储prev和next指针，数组的结尾会预留一定的容量空间

- 两者都是线程不安全的

LinkedList使用场景：

1. 往list中add一个数据，且list已经有几十万个数据时，如果触发扩容则需要复制几十万的数据速度极慢

2. 当需要删除list中的某个数据时（不是指定索引删除，是指定元素），linkedlist也优于arraylist，因为双方都需要遍历获取到元素位置，但arraylist需要挪动后面元素的位置

# **5. CopyOnWriteArrayList**



# 参考
- [红黑树和AVL树的比较](https://blog.csdn.net/zzt4326/article/details/88590731)

- [ConcurrentHashMap7](https://www.cnblogs.com/ITtangtang/p/3948786.html)：HashEntry的next指针域为final类型，每次删除某个元素e时，会将e之前的结点全都克隆一遍（e之后的结点可以复用），并将e之前结点链的队尾结点链接到e之后结点链的第一个结点上。这种做法的思路是：`不变性访问不需要同步从而节省时间`，但是会额外带来删除的复制开销

- [ArrayList和LinkedList区别（蚂蚁金服面试题）](https://cnblogs.com/ysyy/p/10891079.html)

# 重点参考
- [ConcurrentHashMap8](https://blog.csdn.net/ZOKEKAI/article/details/90051567)
