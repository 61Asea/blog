# Netty：线程模型

当netty应用程序（应用层）从**全连接队列**（TCP层接口）中`accpet()`得到`client socket fd`，会通过以下引导代码进行包装：

- 服务器Channel：指定类以初始化服务器channel对象，根据系统来选择

- ChannelHanlder：此处引导先只添加一个初始化childHandler模板，模板用于后续channel的构造，调用`initChannel()`方法来初始化对应的pipeline及其handler链

```java
public static void main(String[] args) {
    // 注意：eventGroup对象生成的同时，其对应的selector也对应生成，至于有多少个selector，则看有多少个eventLoop（此处为1）
    NioEventLoopGroup eventGroup = new NioEventLoopGroup(1);

    ServerBootstrap serverBootstrap = new ServerBootstrap();
    // 1. 设置引导的channel工厂，传入的NioServerSocketChannel作为listen fd的对应，其内部指定NioSocketChannel作为client socket包装初始化时的反射生成类
    serverBoostrap.channel(NioSocketChannel.class)
    // 2. 一般还会添加：TCP选项、netty的worker/boss线程组
    .group(...).option(...)
    // 3. 添加childHandler
    .childHandler(new ChannelInitializer<SocketChannel>() {
        @Override
        public void initChannel(SocketChannel ch) throws Exception {
            ChannelPipeline p = ch.pipeline();
            // 3.1 新增LoggingChannelHandler
            p.addLast("logging", new LoggingHandler(LogLevel.DEBUG));
            if (isProxy) {
                // 3.2 新增HaProxy对应的解码器，同样也是ChannelHandler
                p.addLast(new HAProxyMessageDecoderNew());
            }
            // 3.3 长度解码器
            p.addLast(new LengthFieldBasedFrameDecoder(...));
            // 3.4 根据线程组情况添加自定义的handler
            if (eventGroups.businessGroup != null) {
                p.addLast(eventGroups.businessGroup, handlers);
            } else {
                p.addLast(handlers);
            }
        }
    });
}
```

结合**selector多路复用模型**来分析netty如何构造一个**底层基于jdk.nio**的`朴素Reactor`模型：

- `Handle（listen socket fd、client socket fd）`：文件句柄，直接对应`netty.ServerSocketChannel`和`netty.SocketChannel`

- `Event Handler`：兴趣事件处理器，服务器句柄OP_ACCEPT，客户端句柄OP_READ/OP_WRITE，可直接对应`jdk.nio.Selector#selectionKeys()`中每个key的attachment

    这些事件处理逻辑会以`attachment`的形式包装在一个个`jdk.nio.SelectionKey`中
    
    每当selector阻塞返回时，其对应的SelectionKey集合对应全部就绪的事件，可以很轻松的取出它们连结的attchment直接进行操作

- `Initiation Dispatcher`：兴趣事件管理容器，直接对应`jdk.nio.Selector`

    netty同时运用继承和组合`jdk.nio.Selector`的方式实现`netty.SelectedSelectionKeySetSelector`类
    
    在线程模型上，通过`EventLoopGroup`来隔离每个selector，每个线程组对应一个selector，共有boss和worker两种线程组，可以有多个不同的线程组

    - boss的selector：主要管理listen socket fd的OP_ACCEPT兴趣事件，在bind()、listen()方法调用后就将该事件注册到selector中

    - worker的selector：主要管理每个client socket fd的OP_READ（数据就绪），通过OP_ACCEPT事件的event handler把客户端fd注册到worker的selector中

- `Synchronous Event Demultiplexer`：阻塞句柄实现器，直接对应`netty.SelectedSelectionKeySetSelector#select(long timeout)`方法，底层仍旧是jdk.nio实现

# **服务器的启动流程**

1. boss、worker线程组的初始化，根据预设的线程数N来对应初始化组里的N个eventloop，每个eventloop都有其对应的selector选择器，并共用`ThreadPerTaskExecutor`线程池

    ThreadPerTaskExecutor：基础Executor接口，execute的实现为**每调用一次就新启动一个线程**

2. 引导驱动服务器启动，实例化服务器channel（`pipeline`和`unsafe`在此处被实例化），并通过初始化过程来为其pipeline加入channelInitializer（该方法实现了channelRegistered）

3. 调用boss线程组的register方法，将服务器channel注册进线程组

- 线程组（MultithreadEventLoopGroup）通过`chooser`实现来选择某个eventloop，作为服务器channel的绑定对象
- eventloop的register方法会调用其`unsafe`的register方法
    - channel向其eventloop的selector选择器注册一个值为0的兴趣事件，以此返回SelectionKey并持有该引用
    
    - 触发channel.pipeline的channelRegistered()事件，channelInitializer将为该pipeline新增`ServerBoostrapAcceptor`，并调用eventloop的execute()方法以**通过公共线程池新起线程启动来执行该eventloop的自循环**

        **因为eventloop继承于SingleThreadEventExecutor，流程如下：**

        eventloop.execute() -> 

        SingleThreadEventExecutor.execute(Runnable, boolean) -> 

        SingleThreadEventExecutor.startThread() -> 
        
        SingleThreadEventExecutor.doStartThread() -> **公共线程池.execute()**

    > 截止至此，boss线程组开始运作，一般该线程组只配备一个eventloop

    - 新增OP_ACCEPT兴趣事件到selector中

4. 服务器eventloop自循环阻塞等待selector的返回，当OP_ACCEPT返回时，调用服务器`channel.unsafe.read()`方法（NioMessageUnsafe）

    - 转而调用NioServerSocketChannel.`doReadMessage(List<Object)`方法，通过原生nio的javaChannel来`accept()`客户端连接并生成`NioSocketChannel`，并将连接存储到传入的list\<Object\>中
    - 根据list返回的个数来相对于触发服务器channel的channelRead事件
    - 转入ServerBoostrapAcceptor的channelRead方法，为NioSocketChannel绑定线程组和pipeline的channelInitializer

5. 相当于回到了第三步，只是主角换成了NioSocketChannel，而且此时worker线程组并没有eventloop在执行，仍旧只有boss线程组在执行

6. unsafe.register的`inEventLoop()`判断，发现NioSocketChannel的eventloop对应线程与当前线程不一致（因为worker线程组还未启动，对应线程为null），所以直接驱动eventloop启动线程，并将register0()方法作为任务传入

    > 截止至此，worker线程组的eventloop也开始陆续运作

> 接下来，NioSocketChannel的剩余流程与NioServerSocketChannel如出一辙

![netty线程模型](https://asea-cch.life/upload/2021/11/netty%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B-8cef7a53f14844adb1eea33b538b8cd8.png)

![NioEventLoopGroup执行流程图](https://asea-cch.life/upload/2021/11/NioEventLoopGroup%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE-c772dbe3da764bb1aa55236ef557eda4.png)

# **Netty异步事件驱动模型**

异步是相对于**用户I/O调用层面**上的，网络通信模型依旧是多路复用模型（同步非阻塞I/O），这取决于JDK.NIO

## **ChannelPromise**

> Pin one of the child executors once and remember it so that the same child executor is used to fire events for the same channel：将其中一个子执行器锁定一次并记住它，以便使用同一个子执行器为同一通道触发事件

```java
// DefaultChannelPipeline.java

public class DefaultChannelPipeline {

    // 为pipeline生成各个handler的context时调用
    private AbstractChannelHandlerContext newContext(EventExecutorGroup group, String name, ChannelHandler handler) {
        // 在context中绑定i/o线程，handler作为单例不应该保存状态
        return new DefaultChannelHandlerContext(this, childExecutor(group), name, handler);
    }

    private EventExecutor childExecutor(EventExecutorGroup group) {
        if (group == null) {
            return null;
        }

        Map<EventExecutorGroup, EventExecutor> childExecutors = this.childExecutors;
        if (childExecutors == null) {
            childExecutors = this.childExecutors = new IdentityHashMap<EventExecutorGroup, EventExecutor>(4);
        }

        // I/O线程，也就是NioEventLoop
        EventExecutor childExecutor = childExecutors.get(group);
        if (childExecutor == null) {
            // chooser根据规则顺序选取
            childExecutor = group.next();
            childExecutors.put(group, childExecutor);
        }
        return childExecutor;
    }
}
```

在简单了解完I/O线程、Channel、handler上下文的联系后，我们来分析以下**用户调用I/O操作**的两种情况：

```java
// 数据出站操作
public void sendMsg(ByteBuf msg) {
    ChannelFuture future = channel.writeAndFlush(msg);
    future.addListener((GenericFutureListener) f -> {
        if (f.success()) {
            // ....
        }
    });
}
```

我们可以肯定的是，msg最终必定是通过channel绑定的I/O线程出站的，然而无法确定sendMsg()方法的调用线程，它即可能就是I/O线程，也可能是某一个异步线程，这取决于业务逻辑的实现方式：

- **伪异步**：在netty的I/O线程（NioEventLoop）中处理业务逻辑，常出现于业务代码在ChannelHandler中同步执行

- 异步：在其他线程中处理业务逻辑，常出现于**业务代码只在ChannelHandler中被分发，而不做实际的阻塞调用**

所以，netty作为一个异步事件驱动框架，对于以上两种情况都认为是异步，I/O操作返回的future对象就是对于I/O操作任务的**异步抽象**：

- 伪异步：在调用future.addListener()前，出站操作已经被**当前线程**执行完毕，所以调用addListener时应提供对结果的检查

- 异步：大概率在调用future.addListener()后，出站操作仍为被**异步线程**执行完，所以需要提供任务完成设置结果时的**通知机制**

```java
public class DefaultPromise {
    // 任务完成时会调用该方法设置结果
    private boolean setValue0(Object objResult) {
        if (RESULT_UPDATER.compareAndSet(this, null, objResult) ||
            RESULT_UPDATER.compareAndSet(this, UNCANCELLABLE, objResult)) {
            if (checkNotifyWaiters()) {
                // 异步线程完成任务后的监听器通知
                notifyListeners();
            }
            return true;
        }
        return false;
    }

    @Override
    public Promise<V> addListener(GenericFutureListener<? extends Future<? super V>> listener) {
        checkNotNull(listener, "listener");

        // synchronized串行添加监听器，以确保不会出现并发错误（添加过程中正好任务完成）
        synchronized (this) {
            addListener0(listener);
        }

        // 同步调用的情况
        if (isDone()) {
            notifyListeners();
        }

        return this;
    }

}
```

这种思想与JDK.FutureTask如出一辙，netty的ChannelFuture可以认为是对其的增强改写，提供更方便的回调机制，这也就是Netty为何称为异步事件驱动框架的原因

# 参考
- [线程模型简单总结](https://blog.csdn.net/scying1/article/details/90755438)
- [体会netty对性能的苛刻，EventExecutorChooserFactory](https://www.jianshu.com/p/f2ffff7ee3c9)

# 重点参考
- [Netty中的那些坑](https://www.cnblogs.com/rainy-shurun/p/5213086.html)：讲解netty线程模型下，使用不当导致的各种坑
- [Netty 防止内存泄漏措施](https://www.infoq.cn/article/olLlvGFx*Kr0UV9K7tez)：堆积任务导致问题的分析，包括解决方案