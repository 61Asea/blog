# Classifier分类器 - 李宏毅讲宝可梦和数码宝贝分类

以宝可梦和数码宝贝为例，设计一个图片分类器大模型，对输入的图片进行类型识别

依旧需要用到的有：机器学习三部曲（设计未知函数、定义Loss函数、Loss最优化）、最优化的注意事项（overfitting、critial point、learning rate）

### 前期准备工作

edge detection：边沿探测，可以将图片里的边线保存下来，获得边线图

> 在分类器中，设计的未知参数函数，是以边线的点数高低作为判断分类的方式，所以存在一个阈值h来分割是宝可梦还是数码宝贝（二值化）

函数阈值H：作为类别判断的分割值。也称为模型的复杂程度，可以理解为某个函数他的结果选择性很多。

数据集D：dataset，在分类器中的数据可以抽象为一个参数pair，pair的第一项是分类数据本身，第二项是分类的结果。

### 机器学习三步

1. 定义未知参数函数

h = argminL(h, D)

2. 定义loss函数

error rate(错误率)：将某一个资料集大D的每一笔资料都拿出来，计算每一项与函数阈值H的loss，并所有loss的平均值；loss越大表现越差，反之越好

每一项与H的loss计算：输入类别数据，如果计算结果与数据的识别结果，不一致输出1，一致输出0

error rate的优缺点：
- 优点：直观
- 缺点：不能微分

> 如果遇到需要微分的情况，可以使用cross-entropy来替代error rate

3. 最优化训练

由于在李教授的例子中，H的值是有限可穷举的，可能值由1到10000。所以可以通过穷举计算，不需要使用到gradient desent来计算出最佳参数。

但是虽然H是可穷举的，但是D是不可穷举的。
- 理想状态：如果能获得恒定不变的所有数据集D-all，得到的h-all也将是最优解
- 现实状态：只能获得D-all的子集D-train，训练得到的h-train只能无限接近最优解

目标：希望L(h-train, D-all)能趋近于L(h-all, D-all)