AI基本理论学习：
- 2025年2月
    - 从李宏毅的课程开始入门，首先看的是线性回归相关的一些问题，通过最小二乘法、微分等方式了解了拟合曲线模型、均值差损失，也开始接触到机器学习解决问题的三步（定义模型、定义模型的损失函数，最优化）
    - 从线性回归开始延伸到逻辑回归，并从逻辑回归问题中引出神经网络/深度神经网络的概念
    - 重新学习一些简单的数学概念，涉及到微分（微分、偏微分、复合函数、梯度下降）、线性代数（矩阵向量的相关计算）、概率论（朴素贝叶斯、极大似然），主要看的是书籍《机器学习的数学》
- 2025年3月
    - 思考逻辑回归中为什么要引入sigmoid、ReLU函数（引入生物曲线，将离散值映射到线性回归问题中），以及恰好贝叶斯公式和sigmoid函数间的推导过程，开始了解到交叉熵损失
    - 开始接触经典的机器学习算法，CNN、RNN等
    - 开始看pytorch相关的课程，在b站上的刘二大人《Pytorch深度学习实践》上敲了下代码，基本把上述的线性逻辑回归、经典算法都敲了一遍
    - 开始学习Self-Attention，但是停留在算法表面，没有去看论文深究具体的W_q、W_k、W_v矩阵意义
    - 开始学习Transformer，了解到了encoder、decoder之间的一些区别，并开始学习BERT和GPT的资料
    - 用pytorch跟着b站的视频，手撕了一遍encoder-decoder模式下的transformer
- 2025年4月
    - 希望开始有企业级实践的经验，于是开始看RAG、Agent方向的内容，但是一开始找不到好的学习资料，没有根据LangChain这条路线去学习
    - 开始接触到LangChain，从官方文档+b站的培训课程开始入手，争取整个4月可以尽量建立出一套相对完善的工作框架体系


AI企业级框架学习：
- 第一天：调用了一些openAI相关的接口，配了一些环境变量啥的，主要以熟悉LLM为主

- 第二天：
    - 了解了一下提示词工程的概念，包括经典的工程方法论CRISE（定位、背景上下文、问题、温度）
    - 开始了解LangChain的架构图（LangServer、LangSmith、LangChain-Core、LangChain），注册LangSmith、LangChain、tavily开发需要使用到的API-KEY
    - 简单写了包含链式调用的demo，主要包含了三个最经典的模块: prompt、model、outparser

- 第三天：重新敲了一下第二天学习到的内容，温习了一些LangChain的概念（Message的种类、LangChain的LCEL异步处理语法糖、多种类型的prompt、invoke/stream的使用、stream事件机制）
    - few-shot、example-selector：开始接触example(示例)，了解到example是可以结合运用在提示词模板中的，并可通过selector对问题和示例进行近似度匹配（具体通过向量的余弦近似度计算）。这种方式也可以被视为某种意义上的轻量级RAG
    - LangServer：集成fastApi，pydantic（类似Java的序列化库）
    - history message：通过sessionId或其它自定义组合键，查询历史的聊天记录，并将其加入到prompt中
    - set_llm_cache：通过内存、本地数据库sql-lite等方式，将热点数据进行缓存

- 第四天：多模态的消息prompt、新的json parser（通过BaseModel、Field指定输出格式）、自建工具类相关的内容（装饰器工具、异步工具、同步工具、structured工具创建），顺带感受下toolkit和自带的一下第三方调用工具、最后结合起来试一下带历史记录的聊天agent

- 第五天：开始专门学习embeddings和向量数据库