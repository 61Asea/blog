## 机器学习 - 李宏毅

广义含义：机器具备从数据中学习的能力

标准含义：让机器具备找一个函数的能力

> 假设今天想要让机器做语音识别功能，机器可以听一段声音，产生这段声音对应的文字。那这时候机器其实就具备书写一个转换函数的能力，输入是声音讯号，输出是这段声音讯号的文字内容。

## **函数的类型**

- Regression：输出是一个数值（scale），这样的一个机器学习的任务，称为regression。

    > 预测数值，如输入多个当前的天气参数，输出对接下来天气数值的预测

- Classification：使得机器做选择题，选项称为类别（PLS），并从设定好的选项选择一个作为输出。

    > 做选择题，或者根据提问进行布尔判断。阿尔法狗可抽象为更复杂化的选择题。输入是多个选项，输出是多个选项中的其中一个。

- Structured Learning：结构化内容输出，输出一个图片、文件。

## **如何找函数**

以一个linear model（线性模型）作为例子：

1. Function with Unknown Parameters：先写出一个带有未知参数的函数。（这个过程也称为形成一个**model**）

    based on domain knowledge：根据在领域相关的一些常识，用于这个未知参数函数的初步形成上。

    预测一个youtuber的订阅量。我们可以通过领域常识，了解到今日的订阅量肯定是与明日的点赞量多少有一些指数关联的，然后再猜测是否可以通过一些常数进行修正，最终得出第一个预测数学式：y = b + wx

    - w: weight，权重
    - b: bias，偏正

2. Define Loss from Training Data：从训练集数据中定义出**loss**

    loss：也是一个Function，输入是我们model里的参数(b,w)，输出是当往未知参数w,b设定某个具体数值是好还是不好。

    效果好坏：将第一步的未知参数带入某个具体数值后，再根据训练集资料进行输入、输出，并以此对比效果是否接近已有的训练集数据。

3. Optimization：寻找未知参数的最佳解

    这里通过Gradient descent方法来计算：

    > 注意：通过Gradient descent可以获取到local minima（局部最优解），但不是global minima(全局最优解)。局部最优解其实不是Gradient descent的最大缺点。

    - 随机初始化一个值给到未知参数
    - 计算未知参数的微分（斜率）
    - 根据斜率的正负情况，决定如何移动未知参数的值大小，并由learning rate(学习速率)参数控制移动的步伐
        - 负数：说明切线左高右低，应往右边走，即增大参数
        - 正数：说明切线左低右高，应往左边走，即减小参数
    - 根据设定的次数，来决定未知参数的update次数。每次未知参数移动后，若微分的值越接近于0，则代表误差越小效果更好。

    > 在做机器学习时，需要自己设定的东西，叫hyperparameters

4. 优化模型，将前三步形成的公式进行优化，根据domain knowledge来观察出一些规律进行修改
    
## **优化方案**

### **1. 模型的优化**

当模型在训练的效果不佳时，可以考虑为模型**引入更多的特征**，使得模型变得更加弹性

**Model Bias**：每个模型都是有其相对局限性的，以上述linear model就是一个很好的例子。这种情况称为Model Bias。

#### **1.1 Piecewise linear Curves 分段直线函数**

Piecewise Linear Curves：分段直线函数，具备比linear model**更弹性**的映射关系

分段直线函数可以理解为通过**多个函数+偏移量**，进行组合而成。我们也可以用picewise linear curves，用微积分的方式无限逼近任何的不规则曲线函数，以此达到更具有弹性的效果。

分段函数中的每一段不同的函数，都可以由下述的曲线函数进行拟合
- Sigmoid Function: 曲线函数
- Hard Sigmoid Function: 无限逼近分段直线的曲线函数
- **ReLU**：比sigmoid效果更好的函数，两个ReLu可以代表一个sigmoid函数。

每一个入参都会进入到模型中的所有Sigmoid/ReLU函数进行计算，并将得到的各个子结果进行合并。

#### **1.2 基于深度的模型**

将上述模型的输出结果合并之前的各个子结果，作为新一轮的入参输入到新的sigmoid/ReLU函数里进行计算。

以此作为迭代循环，可以衍生出无数轮计算，迭代的次数我们也称为深度（Layer）。每一层Layer对应的函数计算，我们也称为神经元（Neuron），这一层层的神经元共同组建为类神经网络（Neuron Network）。

这种函数模型对应的机器学习，他由一层层的hidden layer组建而成，具备深度的特性，所以也称为深度学习（Deeping Learning）。

### **2. Optimization Issue优化**

定位是否为optimization较差导致的loss效果不佳？

先从一些弹性比较差，但是optimization求解相对简单的model开始训练，得到一开始的loss。然后再使用更具备更弹性的model进行训练，如果得到的loss没有简单model的loss低，则说明是optimization issue求解没有做好导致的问题。

### **3. Overfitting**

traning data符合layer越大，loss越小的规律，但testing data却loss更大。

一般出现在函数在训练集数据及其准确，但是除开训练集数据以外的位置函数曲线freestyle过于严重，导致与实际情况的特征严重不符。

> 越有弹性的函数，会更容易出现overfitting的情况。上述的函数模型都是属于Fully-connected model，这种架构的特点就是富有弹性。

解决方案：
1. 增加更多的trainning data，使得模型更趋向于真实的函数。
    - data augmentation，用一些对这个问题的理解，创造出更多新的资料。（如影像辨识中，可以将训练资料图片进行放大、缩小、左右镜像等方式，增加训练集大小）
2. 降低模型的弹性，新增一些限制（CNN是比较没有弹性的model，是针对影像的特性来限制模型的弹性）
    - 给予比较少的可变参数
    - 降低更多的神经元数目
3. 降低feature，如之前考虑的输入过多，可以降低一些输入
4. early stopping、regularization、dropout

### **4. Bias-Complexity Trade-off**

模型越来越复杂，training loss则越来越低；testing loss会先下降到一个最低值，然后出现暴增现象（overfitting）

### **5. Cross Validation 如何比较好的规划训练集数据**

将训练集的数据分为两份，一份是真正训练的数据，另一份是用来验证的数据

> 不要太过于在意cargo上的public testing data

存在问题：如果分到验证的validation set很奇怪怎么办

使用N-fold Cross Validation：将数据切成N等分，其中每份数据都既要作为training set，也要作为validation set。

> 假如N=3，则将数据分成A、B、C共计3份，则会出现A-train、B-train、C-val / A-train、B-val、C-train / A-val、B-train、C-train共计三个情况。model在这三个情况下都进行训练得到各个mse，最终再根据Avg mse，这样可以较大程度的减少val set不合理的情况。

## **optimization（最优化）效果差的原因**

### **1. critical point**

可能是gradient（斜率/微分）卡在了critical point上，导致虽然微分结果为0，但是loss的下降不符合预期。

critical point分为以下两类：
- local minima：局部最优解
- saddle point：马鞍点，可以理解为函数模型是三维的，这个点刚好在马鞍三维模型的中心点。在不同平面上并不是loss最低的点，但是微分也为0.

> 区分critical point是否卡在saddle point是有意义的，因为saddle point他可预见在其他平面可以让loss变得更低；而local minima已经是局部的最优解了。

微积分和线形代数来判断是哪种类型，可以使用中值定理公式来表示某个未知参数下的loss函数公式。

Hessian：中值定理公式中的H参数，是一个矩阵。

当卡在critical point时，可以根据Hessian的eigen value（特征值）来判断是哪种类型的point。
- local minima：Hessian矩阵是一个positive definite（正定矩阵），其所有的eigen values都是正的
- saddle point：Hessian矩阵与向量相乘的值有正有负，即eigen values有正有负

### **2. batch size大小的原因**

在机器学习中，批量大小（batch size）是一个超参数，它决定了在更新模型参数之前，模型将多少个训练样本聚合在一起。批量大小的选择对训练速度、内存使用和模型性能有重要影响。

以下是小批量和大批量训练的一些对比：

1. **速度**：

   - **小批量**：对于单个更新，小批量通常更快，因为需要处理的样本更少。但在并行计算中，速度可能相同，因为并行处理可以同时处理多个小批量。
   
   - **大批量**：对于单个更新，大批量可能较慢，因为需要处理更多样本。但在并行计算中，如果批量大小不大到无法并行处理，速度可能相同。

2. **每个epoch的时间**：

   - **小批量**：由于需要更多迭代来完成一个epoch，时间可能更长。
   
   - **大批量**：由于每次迭代处理更多样本，完成一个epoch的时间可能更短。

3. **梯度**：

   - **小批量**：梯度估计可能更加嘈杂，因为它们基于较少的样本。这可能导致训练过程更加不稳定，但有时这种噪声有助于逃离局部最小值。
   
   - **大批量**：梯度估计更加稳定，因为它们基于更多的样本。然而，这可能导致训练过程收敛到较平坦的最小值，可能影响模型的泛化能力。

4. **优化**：

   - **小批量**：可能实现更好的优化，因为噪声梯度有助于探索更广泛的参数空间，避免陷入不良的局部最小值。
   
   - **大批量**：可能优化较差，因为稳定的梯度可能导致过早收敛到局部最小值。

5. **泛化**：

   - **小批量**：通常泛化性能更好，因为噪声梯度有助于找到更宽、更平坦的最小值，这些最小值往往泛化得更好。
   
   - **大批量**：泛化性能可能较差，因为模型可能过度拟合到训练数据的特定最小值。

此外，批量大小的选择还取决于具体的硬件限制和计算资源。在实际应用中，通常选择一个折中的批量大小，以平衡训练速度和模型性能。

请注意，这些是比较一般的观察，实际效果可能因具体模型、数据集和训练设置而异。在实践中，可能需要通过实验来确定最佳批量大小。