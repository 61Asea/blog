## 术语介绍

### **人工智能AI、机器学习ML、深度学习DL的关系**

人工智能、机器学习、深度学习、神经网络 之间可以用一个同心圆来表示：
- 人工智能是最宽泛的概念，涵盖了所有与智能相关的技术和应用
- 机器学习是实现人工智能的一种方法，专注于使机器从数据中学习
- 深度学习是机器学习的一个子集，利用深层神经网络来自动提取特征

### **Prompt**

输入给大模型的文本，用来提示或引导模型给出符合预期的输出

> 提示词，是人与大模型交互的媒介。在日常使用大模型产品时，我们向大模型的提问语句就是prompt。

### **Token**

大模型处理的最小单位，比如英文单词或汉字

Token长度：与大模型交互时使用的单词、汉字数

> 

### **Embedding**

将段落文本编码成固定维度的向量，便于进行语义相似度比较。

> 将领域知识库，向量化为一个向量数据库。这样用户在提问时，可以直接在数据库中提取出相关度较高的材料，给到大模型。

### **Fine-Tuning**

二次训练

> 在已经训练好的模型上，进一步调整模型的过程，是一种使用高质量数据对模型参数进行微调的知识迁移技术，目的是让模型更匹配对*特定任务*的理解。

### **综合举例**

Prompt + Token + Embedding + Fine-Tuning

需求：要把一个设备的使用说明书，做成支持大模型问答的应用。

效果：当用户在使用产品过程中有疑问的话，可以直接向大模型。

步骤：

1. 首先通过Embedding，将这本说明书的文本内容**向量化**为一个向量数据库

    后续用户的提问，可以通过提问内容在数据库中检索出相关内容

2. 将用户的提问，与通过提问在向量数据库检索出来内容，**组合**成一个完整的Prompt

3. 根据Prompt输入，大模型消耗Token。输入的Prompt越多，消耗的Token也就越多。

4. 大模型消耗Token后给出问题的回答，然后再根据Fine-Tuning进行二次训练（根据优质问答数据）。

### **loss**

误差值，一般指的是训练过程中，未知参数给予具体值后，输出的预测值与真实输出的平均误差大小。

L（loss简写）在训练中至关重要，在查找最佳未知参数过程中，loss的斜率往往会更趋向于0，loss越小效果也就越好。

### **hyperparameters**

在机器学习中人设定的参数，而不是机器找出来的参数

- learning rate：参数移动的步伐
- batch size：分批训练集的批次大小
- sigmoid function：分段函数中的每一段拟合函数
- ReLU：用max函数来代表sigmoid，两个ReLu可以变成一个hard sigmoid function

### **Function Model**

### **训练数据**

- training set：训练数据，一般会再分成real traning set和validation set
    - real training set：真正在训练时使用的数据，一般可以占到90%。
    - validation set：用于验证的数据，可以占到10%。也是本地上用于衡量模型的mse分数，后续可以在cargo上传得到对于public testing data的mse
    - batch：分批数据，用于训练过程中，按批次计算model的loss

- public testing set：公开的真实数据，用于在cargo上进行评判模型的数据集
- private testing set：不公开的真实数据，防止有人作弊

### **训练方法**

### **model bias**

指的是训练过程中，模型效果不佳，呈现出来Loss无法很好往下降，这种情况称为模型具有局限性

### **optimization**

属于广义训练过程的最后一个步骤，求解未知参数时，找到能让loss最低的那一组最佳参数。

Optimization issue：指在一些求解方法中（如gradient descent），没有找到全局最优解。