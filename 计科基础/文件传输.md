# IO传输

> [《CPU与内存》](https://asea-cch.life/achrives/硬件模型)：在《CPU与内存》一文中，讲述了CPU与内存之间读写速度差异，以及现代架构以多核共享内存的方式，通过CPU高速缓存来弥补不同硬件的读写差异问题

服务器交互中，可以将数据的交互分为两种，一种为数据操作，一种为纯文件传输

这也意味着，前者的重心在于`数据逻辑处理`，避不可少地需要将数据**读取到用户进程的内存**中；后者重心在于`文件传输`，不需要进行逻辑操作，即不需要读取到**用户进程内存**中

知识点如下：

- 直接内存访问（*direct memory access*）

    包含：网卡（NIC）、磁盘（storage）、CPU拷贝、DMA拷贝

- 内核空间与用户空间

    包含：os用户态与内核态、内核缓冲区（*kernel buffer*）、内核页缓存（*kernel pageCache*）、内核socket缓冲区（*kernel socket buffer*）、用户进程缓冲区（*app buffer*）

- 传统文件传输

    包含：状态切换开销、内存拷贝开销过大的思考
    
- 现代文件传输

    包含：零拷贝

**本文立足点：**

1. **kernel准备数据，从网卡或磁盘中将数据读取到kernel buffer中**

2. **将kernel buffer的数据拷贝到app buffer中**

# **1. 直接内存访问**

DMA，全称*Direct Memory Access*，用于在I/O设备和内存的数据传输工作，又称为DMA拷贝

## **1.1 IO等待过程**

**没有DMA的完整的I/O等待过程，应包含以下几步：**

1. CPU收到请求指令后，发出对应的机器指令到磁盘控制器中，然后返回

2. 磁盘控制器收到指令开始寻址，找到数据扇区后，将数据读取到**磁盘控制器的内部缓冲区**，当缓冲区读满表示已经读取足够多的数据，并产生一个**中断信号**

3. CPU收到中断信号，**停下手头工作**，接着把**磁盘控制器缓冲区**的数据一次一个字节地读进字节的寄存器中，然后再将寄存器的数据写入到**内核数据缓冲区**，在这期间CPU是无法执行其他任务的

4. kernel数据准备就绪，CPU再次拷贝，将数据从内核拷贝到用户空间

## **1.2 DMA优化**

`CPU拷贝`：CPU需要亲自参与到数据搬运的过程中，这种方式称为**CPU拷贝**

> 优化思路：减少CPU拷贝次数，使得CPU可以转而执行其他事务。第三步和第四步都有CPU拷贝，上述过程共有`两次CPU拷贝`的过程，且**第一次CPU拷贝**在大量数据传输下，也都由CPU来进行数据搬运，这将导致CPU利用率急剧下降

`DMA控制器`：用于在进行I/O设备缓冲区和内核缓冲区的数据传输时，负责搬运工作，这样CPU无需参与kernel数据准备阶段，转而执行其他的事务，提高CPU利用率

外设拥有DMA后，优化的I/O等待过程如下：

1. CPU收到请求指令后，发起IO请求到外设的DMA，**剩下过程CPU无需再参与**

    > fd：内核为了高效管理这些已经被打开的文件所创建的索引号，系统用户层可以根据它找到内核层的文件数据

2. 假设是磁盘/网卡的DMA，则进一步将I/O请求发送到磁盘/网卡外设

3. 外设收到DMA的I/O请求后，读取数据到缓冲区中直至缓冲区满，**向DMA发起中断信号**

4. DMA收到中断信号，将**磁盘控制器缓冲区**的数据拷贝到**内核缓冲区**。**注意，这个过程没有占用到CPU，CPU可以执行其他任务**

5. DMA在读取了足够多的数据后，发送中断信号给CPU

    此时fd描述符处于就绪状态，从网卡缓冲区/磁盘缓冲区拷贝过来的数据已放在内核缓冲区中

6. CPU收到DMA中断信号，知道kernel数据准备就绪，于是将数据从内核拷贝到用户空间

    select会遍历所有fd，epoll则取出内核eventpoll事件表，并将数据拷贝到用户空间中，用户态进程再对fd中的数据进行读取

**由此可见，使用DMA优化后，虽然流程多了与DMA交互的两步，但是CPU无需参与kernel数据准备过程的拷贝工作（且是最耗时的第一次拷贝过程），将拷贝次数降至1次**

## **1.3 谁有DMA？**

早期DMA只存在于主板上，现在每个I/O设备里都有自己的DMA控制器

`SG-DMA`：与普通的DMA不同，如果NIC支持该项技术，可以进一步减少通过CPU把内核缓冲区的数据拷贝到socket缓存区的过程

> SG-DMA与sendfile()系统调用是实现零拷贝的关键，下面详解

# **2. 内核空间与用户空间**

> [《CPU与内存第三节OS》](https://asea-cch.life/achrives/cpu)：总结了用户态、内核态、用户态线程和内核态线程的关系

![DMA](https://asea-cch.life/upload/2021/08/DMA-9e417da986834216a398275fa16d8131.jpg)

1. **kernel准备数据，从网卡或磁盘中将数据读取到kernel buffer中**

2. **将kernel buffer的数据拷贝到app buffer中**

## **2.1 内核空间**

OS内核，又称为kernel，具备特权指令的操作权限，并向外部提供系统调用API，辅助用户进程完成硬件操作

`缓存I/O`：使用了内核缓冲区的I/O

### **2.1.1 内核缓冲区（*kernel buffer*）**

> 内核缓冲区，实际上就是磁盘高速缓存（*kernel page cache*），处于内核与磁盘缓冲区之间

 物理介质：内存

优点：
- 弥补内核（内存）与磁盘缓冲区（外存）的读写速度差距，并提供**缓存功能**和**预写功能**，提升**读写磁盘的性能**

    - PageCache：缓存最近被访问的数据，在空间不够时，使用LRU算法淘汰缓存

    - 预读功能：运用局部性原理，尽可能地读取多的数据

-  作为外存和用户内存数据交互的中介者

    - 存放DMA从磁盘缓冲区拷贝过来的文件数据（storage -> kernel）
    - CPU从此处将数据拷贝到用户空间（kernel -> app buffer）

#### **PageCache缺点**

- 通过DMA额外维护PageCache，DMA多做了一次数据拷贝，当PageCache命中率较低时，性能将白白浪费

- 传输大文件（GB级别的文件）时，PageCache不起作用，因为PageCache长时间被大文件占据，其他`热点`的小文件可能就无法充分使用到

    > Mysql使用了冷热分离技术，确保大量读取数据时不会冲刷掉cache buffer的数据

对此，我们的优化思路为**使用异步I/O**

![直接IO解决大文件传输](https://asea-cch.life/upload/2021/08/%E7%9B%B4%E6%8E%A5IO%E8%A7%A3%E5%86%B3%E5%A4%A7%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93-7a1da242048248f7ba213b94313f240d.jpg)

`直接I/O`：绕开PageCache的I/O叫做直接I/O，通常对于磁盘，异步I/O只支持直接I/O

直接 I/O 应用场景常见的两种：

- 应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗
    > 在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启

- 传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O

### **2.1.2 内核socket缓冲区（*kernel socket buffer*）**

> 类似内核缓冲区的思想，是网卡高速缓存，同样处于内核与NIC之间

作用：

**1.** 弥补内核（内存）与网卡（外设）的读写速度差距

**2.** 作为**外网设备**（Client）和用户内存数据交互的中介者，也是套接字的操作区域
> 套接字（socket）：应用层与网络层通信的中间软件抽象层，是一组接口

- 在kernel准备数据期间，存放DMA拷贝的网卡数据（NIC -> kernel socket buffer）

- 在客户端与服务器交互过程中，recv()和send()都通过该区域进行，与TCP/IP协议有重大联系

## **2.2 用户空间**

用户进程缓冲区（*app buffer*），程序操作的内存空间，属于**用户态**范围

# **3. 传统文件传输**

```c++
read(file, tmp_buf, len);

write(socket, tmp_buf, len);
```

![数据逻辑处理的IO交互过程](https://asea-cch.life/upload/2021/08/%E6%95%B0%E6%8D%AE%E9%80%BB%E8%BE%91%E5%A4%84%E7%90%86%E7%9A%84IO%E4%BA%A4%E4%BA%92%E8%BF%87%E7%A8%8B-d5b88a4b2636472eab339658dc47a32e.jpg)

文件传输：将磁盘上的文件读取出来，然后通过网络协议发送到客户端

传统I/O工作方式：
- 数据读取：系统调用read()，包含**两次系统状态的转换，和两次内存拷贝**

    状态转换：第一次，调用read()，从用户态陷入内核态；第二次，DMA拷贝完成，中断CPU拷贝数据到app buffer，从内核态切换回用户态

    内存拷贝：第一次，DMA拷贝磁盘缓冲的数据到kernel中；第二次，CPU拷贝kernel数据到用户空间

- 数据发送/写入：系统调用write(socket, tmp_buf, len)，同样包含**两次系统状态的转换，和两次内存拷贝**

    状态转换：第一次，调用write()，从用户态陷入内核态；第二次，DMA拷贝数据到网卡完成，中断CPU，使得进程从内核态返回用户态

    内存拷贝：第一次，用户进程将数据拷贝到kernel socket buffer；第二次，DMA将数据拷贝到NIC中

传统I/O包含共计**4次**状态转换，**4次**内存拷贝：

`如果在没有数据处理，只单纯传输文件的场景下`，这样的做法将存在冗余的上下文切换和数据拷贝，验证系统性能

# **4. 现代文件传输（对传统模型优化）**

满足单纯传输文件的场景，使用零拷贝进行优化

`零拷贝（Zero-copy）`：避免在内存层面去拷贝数据，也就是说全程没有通过CPU来搬运数据，所有的数据都是通过DMA来进行传输的

## **4.1 sendfile()**

> 立足点：减少状态转换和内存拷贝的次数

状态转换优化：减少系统调用次数

内存拷贝优化：减少不必要的搬运次数，如果只是单纯文件传输，数据完全没必要被拷贝到用户空间与kernel socket buffer中

基于以上的优化思路，提出了零拷贝的优化方案，该技术的实现基于**sendfile**系统调用函数

```c++
# include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

如果网卡不支持`SG-DMA`，sendfile()的作用如下：
- 替代前面的read()和write()两个系统调用，减少一次系统调用的次数，也就减少了两次状态转换

- 可以**直接把内核缓冲区的数据拷贝到socket缓冲区**中，不再拷贝到用户空间内，减少了一次内存拷贝次数

到此处，已经优化成了2次状态转换（**只调用一次sendfile**），3次内存拷贝：

![没有SGDMA的零拷贝](https://asea-cch.life/upload/2021/08/%E6%B2%A1%E6%9C%89SGDMA%E7%9A%84%E9%9B%B6%E6%8B%B7%E8%B4%9D-0da5902751894ee79c82c94d437b486d.jpg)

## **4.2 SG-DMA**

我们在前文讲了对于支持`SG-DMA`的网卡技术，他可以将数据从**kernel buffer直接拷贝到NIC**中，而不需要拷贝到socket缓冲区中。linux内核从2.4版本起，对sendfile()系统调用进行了变化，具体变动就是**支持SG-DMA的功能**：

> 将kernel缓冲区描述符和数据长度传到socket缓冲区后，便直接将数据拷贝到网卡的缓冲区中，减少一次数据拷贝

到此处，已经优化成了2次系统调用，2次内存拷贝：

![零拷贝](https://asea-cch.life/upload/2021/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D-10c2782ca6c4437ab517c707fe1f9dff.jpg)

## **4.3 mmap（只做简单了解）**

sendfile结合SG-DMA主要是减少了在**纯文件传输**场景下的内存拷贝次数，不满足需要操作数据的场景。后者场景下，我们可以通过**mmap**的内存映射方式进行优化

> [mmap实现原理浅析](https://zhuanlan.zhihu.com/p/69555454)

`原理`：**写时复制**思想，当多个进程需要访问文件时，每个进程都将文件所存储的kernel buffer映射到自己的进程地址空间，这个文件对于多个进程而言是`共享映射`的；当多个进程需要写文件时，写操作并不能真正修改对应的磁盘文件，而是退化成匿名映射，并通过**写时拷贝**的方式进行覆盖修改

> [《浅谈mmap》](https://blog.csdn.net/qihoo_tech/article/details/102965775)

`操作流程`：open一个文件（仍然也是先读到page cache中），然后调用mmap系统调用，将返回的虚拟地址最终对应的页表内容设定为和前面page cache相同的物理页

> 这样，文件的内容的全部或一部分就可以直接通过mmap映射到进程的虚拟地址空间，避免了第二次拷贝，mmap并不分配真正的物理地址空间，无需预先分配好物理内存

`共享磁盘文件机制（流程）`：
1. 当有第一个进程访问kernel buffer时，由于并没有实际拷贝数据，这时MMU在地址映射表中是无法找到与地址空间相对应的物理地址，即MMU失败，触发`缺页中断`

2. 中断后，内核将文件的这一页数据读入到kernel buffer，并更新进程页表，使得页表指向kernel buffer对应的页，之后有其它进程再次访问这一页时，该页就已经在内存中了，内核只需要将进程的页表登记并且指向内核的页高速缓冲区即可

> [《认真分析mmap：是什么 为什么 怎么用》](https://www.cnblogs.com/huxiao-tee/p/4660352.html)：深度好文

以前的epoll模型是使用了mmap技术的，后期优化后将mmap技术去除

# **5. I/O五大模型**

通过以上的总结，我们后续可以引出下文5大IO模型的总结：

> [《IO模型》](https://asea-cch.life/achrives/IO模型)：讲述了5种linux的IO模型，它们不仅用于本地磁盘文件I/O，同样也被广泛应用于网络I/O上，是文件传输的重要思想模型

# **6. 总结**

**PageCache和零拷贝相辅相成**：零拷贝将内存复制的流程降至2次，其中的第一次DMA拷贝到kernel缓冲区，PageCache以其介质与机制，进一步提升该拷贝过程的速度

**传输大文件的思路**：使用异步I/O（直接IO）的方式，避免PageCache被污染

**传输小数据的思路**：使用零拷贝技术

## **零拷贝总结**

没有零拷贝技术，则需要面临**4次上下文切换（一个系统调用两个切换，read和write则为4个）**和**4次内存拷贝**：

> 2次数据拷贝发送在内存缓存区和对应的硬件设备之间（由DMA完成），另外2次则发送在内核态和用户态之间（由CPU完成）

- 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的
- 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的
- 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的
- 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的

使用`零拷贝技术`：通过一次系统调用（sendfile），合并了磁盘读取和网络发送两个操作，去除了数据在内核态和用户态之间的拷贝，即CPU无需再参与该过程

结论：所以，最后只需要**2次上下文切换**和**2次数据拷贝**

# 参考
- [最透彻的五种linux IO模型分析](https://zhuanlan.zhihu.com/p/393635611?utm_source=ZHShareTargetIDMore&utm_medium=social&utm_oi=793369254232731648)
- [《CPU与内存》](https://asea-cch.life/achrives/硬件模型)：在《CPU与内存》一文中，讲述了CPU与内存之间读写速度差异，以及现代架构以多核共享内存的方式，通过CPU高速缓存来弥补不同硬件的读写差异问题
- [《IO模型》](https://asea-cch.life/achrives/IO模型)：讲述了5种linux的IO模型，它们不仅用于本地磁盘文件I/O，同样也被广泛应用于网络I/O上，是文件传输的重要思想模型

# 重点参考
- [零拷贝](https://zhuanlan.zhihu.com/p/258513662)
- [《认真分析mmap：是什么 为什么 怎么用》](https://www.cnblogs.com/huxiao-tee/p/4660352.html)